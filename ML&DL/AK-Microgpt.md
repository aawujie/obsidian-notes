è¿™æ˜¯ä¸€ç¯‡å…³äºŽ Andrej Karpathy è¿™æ®µâ€œåŽŸå­çº§â€GPT ä»£ç çš„å­¦ä¹ ç¬”è®°ã€‚

---

# ðŸ“ ã€ç¡¬æ ¸æ‹†è§£ã€‘å¾’æ‰‹æ“ GPTï¼šKarpathy çš„â€œåŽŸå­çº§â€Python ä»£ç è§£è¯»

> **æ ¸å¿ƒçœ‹ç‚¹**ï¼šè¿™æ®µä»£ç å±•ç¤ºäº†åœ¨æ²¡æœ‰ä»»ä½•æ·±åº¦å­¦ä¹ æ¡†æž¶ï¼ˆå¦‚ PyTorch/TensorFlowï¼‰å’ŒçŸ©é˜µè¿ç®—åº“ï¼ˆå¦‚ NumPyï¼‰çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä»…ç”¨ Python æ ‡å‡†åº“ï¼ˆ`math`, `random`ï¼‰ä»Žé›¶æž„å»ºä¸€ä¸ªèƒ½å¤Ÿè®­ç»ƒå’ŒæŽ¨ç†çš„ GPT æ¨¡åž‹ã€‚
>
> **ä¸€å¥è¯æ€»ç»“**ï¼š**ä¸€åˆ‡çš†ä¸ºè®¡ç®—å›¾ï¼Œä¸€åˆ‡çš†ä¸ºé“¾å¼æ³•åˆ™ã€‚**
>
> å¯è§†åŒ–è®²è§£ï¼šhttps://microgpt.enescang.dev/

---

## 1. åœ°åŸºï¼šæ‰‹æ“è‡ªåŠ¨å¾®åˆ†å¼•æ“Ž (`class Value`)

è¿™æ˜¯æ•´ä¸ªä»£ç çš„çµé­‚ã€‚Karpathy åœ¨è¿™é‡Œå¤çŽ°äº†ä¸€ä¸ªå¾®åž‹çš„ Autograd å¼•æ“Žï¼ˆç±»ä¼¼ä»–çš„ `micrograd` é¡¹ç›®ï¼‰ã€‚

*   **è®¾è®¡å“²å­¦**ï¼šä¸ºäº†èƒ½è®­ç»ƒç¥žç»ç½‘ç»œï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“è°ƒæ•´å“ªä¸ªå‚æ•°èƒ½é™ä½Ž Lossã€‚è¿™éœ€è¦**åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰**ã€‚
*   **å®žçŽ°ç»†èŠ‚**ï¼š
    * æ¯ä¸ª `Value` å¯¹è±¡åŒ…è£…ä¸€ä¸ªæ ‡é‡æ•°æ®ï¼ˆ`data`ï¼‰å’Œå®ƒçš„æ¢¯åº¦ï¼ˆ`grad`ï¼‰ã€‚
    *   **è¿ç®—é‡è½½**ï¼šé‡å†™äº† `__add__`ï¼ˆåŠ ï¼‰ã€`__mul__`ï¼ˆä¹˜ï¼‰ã€`relu`ã€`log` ç­‰è¿ç®—ã€‚å½“ä½ åš `c = a * b` æ—¶ï¼Œå®ƒä¸ä»…è®¡ç®—ç»“æžœï¼Œè¿˜è®°ä¸‹äº† `a` å’Œ `b` æ˜¯ `c` çš„â€œå­©å­â€ï¼Œå¹¶è®°ä¸‹äº†å±€éƒ¨å¯¼æ•°ï¼ˆ`_local_grads`ï¼‰ã€‚
    *   **æ‹“æ‰‘æŽ’åºä¸Žåå‘ä¼ æ’­**ï¼š`backward()` å‡½æ•°é€šè¿‡**æ‹“æ‰‘æŽ’åº**ï¼ˆTopological Sortï¼‰ç¡®ä¿åœ¨è®¡ç®—æŸä¸ªèŠ‚ç‚¹çš„æ¢¯åº¦å‰ï¼Œå®ƒçš„æ‰€æœ‰åŽç»§èŠ‚ç‚¹çš„æ¢¯åº¦éƒ½å·²ç»è®¡ç®—å®Œæ¯•ï¼Œç„¶åŽé€šè¿‡**é“¾å¼æ³•åˆ™**é€’å½’å›žä¼ æ¢¯åº¦ã€‚

## 2. æ•°æ®å‡†å¤‡ï¼šå­—ç¬¦çº§ Tokenizer

è¿™å°±å¥½æ¯”è®© AI å­¦è¯´è¯å‰å…ˆè®¤å­—ã€‚

*   **Tokenizer**ï¼šè¿™é‡Œéžå¸¸ç®€å•ï¼Œæ˜¯**å­—ç¬¦çº§ï¼ˆCharacter-levelï¼‰**ã€‚
    * è¯è¡¨ï¼ˆVocabï¼‰ï¼šæ•°æ®é›†ä¸­æ‰€æœ‰å‡ºçŽ°è¿‡çš„å”¯ä¸€å­—ç¬¦ + 1 ä¸ªç‰¹æ®Šçš„ `BOS`ï¼ˆBeginning of Sequenceï¼‰æ ‡è®°ã€‚
    * è¾“å…¥ï¼š`['a', 'b', 'c']` $\rightarrow$ `[1, 2, 3]`ã€‚

## 3. éª¨æž¶ï¼šGPT æ¨¡åž‹æž¶æž„

å‡½æ•° `gpt(...)` å®šä¹‰äº†æ¨¡åž‹çš„å‰å‘ä¼ æ’­é€»è¾‘ã€‚è™½ç„¶ä»£ç çŸ­ï¼Œä½†å®ƒåŒ…å«äº† Transformer çš„æ‰€æœ‰å…³é”®ç»„ä»¶ï¼ˆé™¤äº† Dropout å’Œ Biasï¼‰ã€‚

### 3.1 åµŒå…¥å±‚ (Embeddings)

*   **Token Embedding (`wte`)**ï¼šæŠŠå­—å˜æˆå‘é‡ã€‚
*   **Position Embedding (`wpe`)**ï¼šGPT éœ€è¦çŸ¥é“â€œç¬¬å‡ ä¸ªå­—â€ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼Œè€Œä¸æ˜¯æ­£å¼¦ä½ç½®ç¼–ç ã€‚
*   **è¾“å…¥** = Token å‘é‡ + ä½ç½®å‘é‡ã€‚

### 3.2 Transformer Block (å±‚)

è¿™é‡Œé€šè¿‡å¾ªçŽ¯ `n_layer` æ¬¡æ¥å †å å±‚ã€‚æ¯å±‚åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒå­å±‚ï¼š

1.  **å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Attention)**ï¼š
    *   **Q, K, V**ï¼šé€šè¿‡çº¿æ€§å±‚ï¼ˆ`linear`ï¼‰ç”Ÿæˆ Query, Key, Valueã€‚
    *   **KV-Cache**ï¼šæ³¨æ„ä»£ç ä¸­çš„ `keys[li].append(k)` å’Œ `values[li].append(v)`ã€‚è¿™æ˜¯ä¸€ä¸ªéžå¸¸æœ‰è¶£çš„ç»†èŠ‚ï¼é€šå¸¸è®­ç»ƒæ—¶æˆ‘ä»¬ä¼šåˆ©ç”¨çŸ©é˜µå¹¶è¡Œä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ Masked Attentionï¼Œä½†è¿™é‡Œä¸ºäº†ä»£ç é€»è¾‘ç»Ÿä¸€ï¼ˆä¸”å› ä¸ºæ²¡æœ‰çŸ©é˜µåº“ï¼‰ï¼ŒKarpathy é€‰æ‹©äº†**ä¸²è¡Œè®­ç»ƒ**ï¼ˆä¸€ä¸ª Token ä¸€ä¸ª Token åœ°å–‚ï¼‰ï¼Œå› æ­¤è®­ç»ƒè¿‡ç¨‹ä¹Ÿä½¿ç”¨äº†æŽ¨ç†æ—¶å¸¸ç”¨çš„ KV Cache æŠ€æœ¯ã€‚
    *   **Attention å…¬å¼**ï¼š$\text{softmax}(\frac{Q K^T}{\sqrt{d_k}}) V$ã€‚ä»£ç é‡Œç”¨åˆ—è¡¨æŽ¨å¯¼å¼ç¡¬å†™äº†è¿™ä¸ªæ•°å­¦å…¬å¼ã€‚
2.  **å‰é¦ˆç¥žç»ç½‘ç»œ (MLP)**ï¼š
    * ç»“æž„ï¼šLinear $\rightarrow$ ReLU $\rightarrow$ Linearã€‚
    * ä½œç”¨ï¼šå¢žåŠ éžçº¿æ€§ï¼Œæ•´åˆç‰¹å¾ã€‚

### 3.3 ç»†èŠ‚å¤„ç†

*   **RMSNorm**ï¼šä½¿ç”¨äº† Root Mean Square Layer Normalizationï¼ˆåŽ»é™¤äº†å‡å€¼é¡¹ï¼Œæ¯”æ ‡å‡† LayerNorm æ›´ç®€å•ä¸”æ•ˆæžœç›¸å½“ï¼‰ï¼Œç”¨äºŽç¨³å®šè®­ç»ƒã€‚
*   **æ®‹å·®è¿žæŽ¥ (Residual Connection)**ï¼š`x = [a + b for a, b in zip(x, x_residual)]`ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œè®©ç½‘ç»œèƒ½åšå¾—æ›´æ·±ã€‚

## 4. è®­ç»ƒå¾ªçŽ¯ï¼šæžç®€çš„ Adam ä¼˜åŒ–å™¨

è¿™éƒ¨åˆ†ä»£ç å±•ç¤ºäº†å¦‚ä½•åœ¨æ²¡æœ‰ PyTorch `optim` æ¨¡å—çš„æƒ…å†µä¸‹æ›´æ–°å‚æ•°ã€‚

*   **æŸå¤±å‡½æ•°**ï¼š**è´Ÿå¯¹æ•°ä¼¼ç„¶ (Negative Log Likelihood)**ã€‚
    *   `loss_t = -probs[target_id].log()`ã€‚ç›®æ ‡æ¦‚çŽ‡è¶Šå¤§ï¼ŒLoss è¶Šå°ã€‚
*   **æ‰‹åŠ¨ Adam**ï¼š
    *   Adam éœ€è¦ç»´æŠ¤ä¸¤ä¸ªåŠ¨é‡ï¼šä¸€é˜¶çŸ© `m`ï¼ˆæ¢¯åº¦çš„ç§»åŠ¨å¹³å‡ï¼‰å’ŒäºŒé˜¶çŸ© `v`ï¼ˆæ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡ï¼‰ã€‚
    * ä»£ç æ‰‹åŠ¨å®žçŽ°äº† Adam çš„æ›´æ–°å…¬å¼ï¼š
        $$ \theta_{t+1} = \theta_t - \eta \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon} $$
    * åŒ…å« Bias correctionï¼ˆåå·®ä¿®æ­£ï¼‰å’Œå­¦ä¹ çŽ‡è¡°å‡ã€‚

## 5. æŽ¨ç† (Inference)

æ¨¡åž‹è®­ç»ƒå®ŒåŽï¼Œâ€œåšæ¢¦â€çš„è¿‡ç¨‹ã€‚

*   **Temperature**ï¼šæŽ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ã€‚
    *   `logits / temperature`ã€‚æ¸©åº¦è¶Šä½Žï¼Œåˆ†å¸ƒè¶Šå°–é”ï¼ˆè¶Šä¿å®ˆï¼‰ï¼›æ¸©åº¦è¶Šé«˜ï¼Œåˆ†å¸ƒè¶Šå¹³ç¼“ï¼ˆè¶Šèƒ¡è¯´å…«é“ï¼‰ã€‚
*   **Sampling**ï¼šä½¿ç”¨ `random.choices` æ ¹æ®æ¦‚çŽ‡åˆ†å¸ƒé‡‡æ ·ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œç›´åˆ°é‡åˆ° `BOS` ç»“æŸã€‚

---

## ðŸ’¡ æ·±åº¦æ€è€ƒï¼šè¿™æ®µä»£ç å‘Šè¯‰äº†æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ

1.  **åŽ»é­…**ï¼šChatGPT çœ‹èµ·æ¥é«˜æ·±èŽ«æµ‹ï¼Œä½†æ‹†è§£åˆ°æœ€åº•å±‚ï¼Œå°±æ˜¯åŠ å‡ä¹˜é™¤ã€æŒ‡æ•°å¯¹æ•°ï¼ŒåŠ ä¸Šé“¾å¼æ³•åˆ™æ±‚å¯¼ã€‚æ²¡æœ‰é­”æ³•ï¼Œåªæœ‰æ•°å­¦ã€‚
2.  **æ•ˆçŽ‡ vs åŽŸç†**ï¼šè¿™æ®µä»£ç çš„æ•ˆçŽ‡æžä½Žï¼ˆçº¯ Python å¾ªçŽ¯ï¼Œæ— å‘é‡åŒ–ï¼‰ï¼Œåœ¨çŽ°ä»£ GPU ä¸Šè®­ç»ƒå¤§æ¨¡åž‹æ—¶ï¼Œæˆ‘ä»¬ä¼šæŠŠè¿™äº›å¾ªçŽ¯å…¨éƒ¨å˜æˆçŸ©é˜µä¹˜æ³•ï¼ˆMatrix Multiplicationï¼‰ã€‚Karpathy åœ¨å¼€å¤´æ³¨é‡Šå†™é“ï¼š*Everything else is just efficiency*ï¼ˆå…¶ä½™çš„åªæ˜¯æ•ˆçŽ‡é—®é¢˜ï¼‰ã€‚
3.  **ä¸²è¡Œè®­ç»ƒçš„ä»£ä»·**ï¼šæ³¨æ„çœ‹è®­ç»ƒå¾ªçŽ¯ `for pos_id in range(n)`ã€‚æ ‡å‡†çš„ GPT è®­ç»ƒæ˜¯å¹¶è¡Œçš„ï¼ˆä¸€æ¬¡æ€§é¢„æµ‹æ‰€æœ‰ä½ç½®çš„ä¸‹ä¸€ä¸ª Tokenï¼‰ï¼Œè€Œè¿™é‡Œä¸ºäº†ä»£ç å¯è¯»æ€§å’Œâ€œåŽŸå­æ€§â€ï¼Œé‡‡ç”¨äº†ä¸²è¡Œé¢„æµ‹ã€‚è¿™æ„å‘³ç€å®ƒçš„è®­ç»ƒé€Ÿåº¦æ˜¯ $O(N)$ è€Œä¸æ˜¯å¹¶è¡Œä¸‹çš„ $O(1)$ (å°±æ—¶é—´æ­¥è€Œè¨€)ã€‚

## Code

```python
The most atomic way to train and inference a GPT in pure, dependency-free Python.
This file is the complete algorithm.
Everything else is just efficiency.

@karpathy
"""

import os       # os.path.exists
import math     # math.log, math.exp
import random   # random.seed, random.choices, random.gauss, random.shuffle
random.seed(42) # Let there be order among chaos

# Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)
if not os.path.exists('input.txt'):
    import urllib.request
    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'
    urllib.request.urlretrieve(names_url, 'input.txt')
docs = [l.strip() for l in open('input.txt').read().strip().split('\n') if l.strip()] # list[str] of documents
random.shuffle(docs)
print(f"num docs: {len(docs)}")

# Let there be a Tokenizer to translate strings to discrete symbols and back
uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1
BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token
vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS
print(f"vocab size: {vocab_size}")

# Let there be Autograd, to recursively apply the chain rule through a computation graph
class Value:
    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage

    def __init__(self, data, children=(), local_grads=()):
        self.data = data                # scalar value of this node calculated during forward pass
        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass
        self._children = children       # children of this node in the computation graph
        self._local_grads = local_grads # local derivative of this node w.r.t. its children

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data + other.data, (self, other), (1, 1))

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data * other.data, (self, other), (other.data, self.data))

    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))
    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))
    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))
    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))
    def __neg__(self): return self * -1
    def __radd__(self, other): return self + other
    def __sub__(self, other): return self + (-other)
    def __rsub__(self, other): return other + (-self)
    def __rmul__(self, other): return self * other
    def __truediv__(self, other): return self * other**-1
    def __rtruediv__(self, other): return other * self**-1

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        self.grad = 1
        for v in reversed(topo):
            for child, local_grad in zip(v._children, v._local_grads):
                child.grad += local_grad * v.grad

# Initialize the parameters, to store the knowledge of the model.
n_embd = 16     # embedding dimension
n_head = 4      # number of attention heads
n_layer = 1     # number of layers
block_size = 16 # maximum sequence length
head_dim = n_embd // n_head # dimension of each head
matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]
state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}
for i in range(n_layer):
    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)
params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]
print(f"num params: {len(params)}")

# Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.
# Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU
def linear(x, w):
    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]

def softmax(logits):
    max_val = max(val.data for val in logits)
    exps = [(val - max_val).exp() for val in logits]
    total = sum(exps)
    return [e / total for e in exps]

def rmsnorm(x):
    ms = sum(xi * xi for xi in x) / len(x)
    scale = (ms + 1e-5) ** -0.5
    return [xi * scale for xi in x]

def gpt(token_id, pos_id, keys, values):
    tok_emb = state_dict['wte'][token_id] # token embedding
    pos_emb = state_dict['wpe'][pos_id] # position embedding
    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding
    x = rmsnorm(x)

    for li in range(n_layer):
        # 1) Multi-head attention block
        x_residual = x
        x = rmsnorm(x)
        q = linear(x, state_dict[f'layer{li}.attn_wq'])
        k = linear(x, state_dict[f'layer{li}.attn_wk'])
        v = linear(x, state_dict[f'layer{li}.attn_wv'])
        keys[li].append(k)
        values[li].append(v)
        x_attn = []
        for h in range(n_head):
            hs = h * head_dim
            q_h = q[hs:hs+head_dim]
            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]
            v_h = [vi[hs:hs+head_dim] for vi in values[li]]
            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]
            attn_weights = softmax(attn_logits)
            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]
            x_attn.extend(head_out)
        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])
        x = [a + b for a, b in zip(x, x_residual)]
        # 2) MLP block
        x_residual = x
        x = rmsnorm(x)
        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])
        x = [xi.relu() for xi in x]
        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])
        x = [a + b for a, b in zip(x, x_residual)]

    logits = linear(x, state_dict['lm_head'])
    return logits

# Let there be Adam, the blessed optimizer and its buffers
learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8
m = [0.0] * len(params) # first moment buffer
v = [0.0] * len(params) # second moment buffer

# Repeat in sequence
num_steps = 1000 # number of training steps
for step in range(num_steps):

    # Take single document, tokenize it, surround it with BOS special token on both sides
    doc = docs[step % len(docs)]
    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]
    n = min(block_size, len(tokens) - 1)

    # Forward the token sequence through the model, building up the computation graph all the way to the loss.
    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
    losses = []
    for pos_id in range(n):
        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]
        logits = gpt(token_id, pos_id, keys, values)
        probs = softmax(logits)
        loss_t = -probs[target_id].log()
        losses.append(loss_t)
    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.

    # Backward the loss, calculating the gradients with respect to all model parameters.
    loss.backward()

    # Adam optimizer update: update the model parameters based on the corresponding gradients.
    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay
    for i, p in enumerate(params):
        m[i] = beta1 * m[i] + (1 - beta1) * p.grad
        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2
        m_hat = m[i] / (1 - beta1 ** (step + 1))
        v_hat = v[i] / (1 - beta2 ** (step + 1))
        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)
        p.grad = 0

    print(f"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}")

# Inference: may the model babble back to us
temperature = 0.5 # in (0, 1], control the "creativity" of generated text, low to high
print("\n--- inference (new, hallucinated names) ---")
for sample_idx in range(20):
    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
    token_id = BOS
    sample = []
    for pos_id in range(block_size):
        logits = gpt(token_id, pos_id, keys, values)
        probs = softmax([l / temperature for l in logits])
        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]
        if token_id == BOS:
            break
        sample.append(uchars[token_id])
    print(f"sample {sample_idx+1:2d}: {''.join(sample)}")
```
